{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\slcf\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.1 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/score_발열.xlsx\"\n",
    "sheet_name = \"Sheet1\"\n",
    "data = pd.read_excel(filename, sheet_name = sheet_name, header = 0)\n",
    "\n",
    "csv_data = [item.replace(\"#\", \"\").strip() for item in data['Review']]\n",
    "csv_label = data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['발열히 심한거 같은데 여름이라 그런가?..',\n",
       " '발열이좀 심한거 같아서 걱정이에요',\n",
       " '발열이심하더라구요',\n",
       " '발열이너무심한게 제일큰 단점인것 같고 그외에 불편한점은',\n",
       " '발열이...정말...심합니다']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.2 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()\n",
    "size = 500\n",
    "\n",
    "doc = []\n",
    "\n",
    "for sentence in csv_data :\n",
    "    results= []\n",
    "    tokens = twitter.pos(sentence, norm=True, stem=True)\n",
    "        \n",
    "    for token in tokens:\n",
    "        if not token[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            results.append(token[0])\n",
    "    doc.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['발열', '히', '심하다', '같다', '여름', '그', '런가'],\n",
       " ['발열', '이', '좀', '심하다', '같다', '걱정'],\n",
       " ['발열', '심하다'],\n",
       " ['발열', '이', '너무', '심하다', '제일', '크다', '단점', '것', '같다', '그', '외', '불편하다', '점'],\n",
       " ['발열', '정말', '심하다']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(doc, size=size, window=2, hs=0, min_count=3, sg=0)\n",
    "\n",
    "w2v = dict(zip(model.wv.index2word, model.wv.vectors))\n",
    "\n",
    "del model\n",
    "\n",
    "# sentences (iterable of iterables, optional) – The sentences iterable can be simply a list of lists of tokens, but for larger corpora, consider an iterable that streams the sentences directly from disk/network. See BrownCorpus, Text8Corpus or LineSentence in word2vec module for such examples. See also the tutorial on data streaming in Python. If you don’t supply sentences, the model is left uninitialized – use if you plan to initialize it in some other way.\n",
    "# size (int, optional) – Dimensionality of the word vectors.\n",
    "# window (int, optional) – Maximum distance between the current and predicted word within a sentence.\n",
    "# min_count (int, optional) – Ignores all words with total frequency lower than this.\n",
    "# workers (int, optional) – Use these many worker threads to train the model (=faster training with multicore machines).\n",
    "# sg ({0, 1}, optional) – Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "# hs ({0, 1}, optional) – If 1, hierarchical softmax will be used for model training. If 0, and negative is non-zero, negative sampling will be used.\n",
    "# negative (int, optional) – If > 0, negative sampling will be used, the int for negative specifies how many “noise words” should be drawn (usually between 5-20). If set to 0, no negative sampling is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['발열', '없다', '하다', '있다', '소음', '심하다', '좋다', '같다', '않다', '거의', '좀', '적다', '자다', '것', '잡다', '되다', '만족', '문제', '조금', '정도', '생각', '괜찮다', '못', '사용', '노트북', '이', '너무', '느끼다', '이다', '부분', '팬', '성능', '쿨러', '정말', '크다', '돌아가다', '잘', '제품', '걱정', '안', '배터리', '별로', '도', '더', '전혀', '속도', '게임', '많이', '쓰다', '매우', '쓸다', '아주', '가볍다', '소리', '어쩌다', '아직', '느껴지다', '보다', '적', '다', '그렇다', '때문', '나다', '많다', '키', '편이', '관리', '아니다', '보이다', '신경', '크게', '조용하다', '약간', '때', '오래', '및', '네', '그', '상당하다', '수', '들다', '없이', '거', '꽤', '보드', '시간', '양호', '쿨링', '무게', '아쉽다', '디자인', '제어', '점', '심해', '높다', '빠르다', '비', '또한', '모두', '요', '빼다', '돌리다', '모르다', '발생', '삼성', '다른', '감', '맘', '감다', '잘되다', '잡히다', '확실하다', '진짜', '하지만', '들', '해보다', '가격', '편', '모델', '안나', '가다', '개선', '단점', '느낌', '제', '뜨겁다', '펜', '하나', '시', '듯', '사은', '수준', '최고', '만족스럽다', '10', '2', '지다', '구매', '습', '되어다', '메탈', '약하다', '상태', '충전', '굉장하다', '한', '뜨다', '훨씬', '품', '작업', '화면', '발', '열량', '아', '그렇게', '장시간', '사', '생기다', '중', '무엇', '3', '인', '걸리다', '받침', '대', '구성', '깔끔하다', '주다', '저', '인터넷', '그래픽', '패드', '이상', '그램', '기능', '불편하다', '프로그램', '심', '마음', '듭니', '막', '부팅', '분', '느리다', '센터', '사양', '낮다', '지금', '빠지다', '50', '처리', '알', '어느', '배송', '그래도', '따다', '성', '필요', '돌다', '아예', '나오다', '데스크탑', '70', '따뜻하다', '카드', '펴다', '고사', '양', '엄청', '완벽하다', '오다', '안심', '대비', '하니', '알다', '가성', '우수하다', '선택', '1', '덜', '작다', '패널', '램', '용량', '어마', '끊기다', '이렇다', '원래', '살짝', '화이트', '내다', '차이', '하판', '고민', '내', '이쁘다', '현상', '100', '장점', '일어나서', '문서', '편입', '니', '컴퓨터', '이정', '뜻', '편하다', '이면', '봐', '받다', '뭐', '등등', '후', '사다', '설치', '쓰이다', 'ㅜㅜ', '등', '모니터', '던지다', '추천', '그리다', '대다', '에서', '딱하다', '틀림', '모든', '완전', '까지', '딱', '뛰어나다', '준수', '특히', '물론', '역시', '이슈', '히', '여름', '외', 'ㅠㅠ', '한편', '심다', '밧데리', '년', '자판', '이전', '불량', '심해지다', '스', '은', '라면', '화상', '야하다', '제외', '자주', '5', '치다', '소모', '하드', '왼쪽', '비추다', '휴대', '다니다', '9', '늘다', '이제', '조절', '잡지', '음', '다소', '따르다', '4', '어떻다', '켜다', '능력', '떨어지다', '손', '임', '옆', '돼다', '그냥', '절대', '한성', '얇다', '일이', '해소', '쫌', '기본', 'S', '서비스', '정', '나', '두다', '온도', '예쁘다', '겨울', '나쁘다', '액정', '레노버', '처음', '얘기', '연결', '작성', 'ㅎ', '평소', '노트', '곳', '별', 'SSD', '마감', '윈도우', '7', '추가', '보통', '차다', '빠릿빠릿', '적당하다', '굿', '무겁다', '대만', '족', '줄', 'i', '짧다', '뒤', '면', '기다', '증상', '현재', '화질', 'ssd', '이렇게', '화', '상', '금', '놀라다', '점수', '기', '무리', '외관', '대단하다', '미미', '저렴', '잡고', '엄청나다', '배그', 'ips', '할인'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document to 3-dim Matrix with Word2Vector & Get Max Length of Sentence\n",
    "\n",
    "doc2vec = []\n",
    "max_length = 0\n",
    "\n",
    "for sentence in doc :\n",
    "    temp = []\n",
    "    length = 0\n",
    "    \n",
    "    for word in sentence :\n",
    "        if word in w2v.keys() :\n",
    "            temp.append(w2v[word])\n",
    "            length += 1\n",
    "            \n",
    "    doc2vec.append(temp)\n",
    "    \n",
    "    if max_length <= length :\n",
    "        max_length = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill zeros for fitting size\n",
    "\n",
    "for sentence in doc2vec :\n",
    "    \n",
    "    length = len(sentence)\n",
    "    \n",
    "    while length < max_length :\n",
    "        sentence.append(np.zeros(size))\n",
    "        length += 1\n",
    "\n",
    "doc2vec = np.array(doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211, 12, 500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = doc2vec\n",
    "label = csv_label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.3 Train-Test Spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(train_data).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(train_label).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([908]), torch.Size([908, 12, 500]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size() , x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(-1, 1, 12, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_utils.TensorDataset(x, y)\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1,\n",
    "                                          drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 12, 500])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, label = iter(train_loader).next()\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.4 Define Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1,16,3), #1*12*500 -> 16*10*498\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,3), #16*10*498 -> 32*8*496\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2), #32*8*496 -> 32*4*248\n",
    "            nn.Conv2d(32,64,3),#32*4*248 -> 64*2*246\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2) #64*2*246 -> 64*1*123\n",
    "        )\n",
    "        \n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(64*1*123,100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,3)\n",
    "        )       \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(-1,64*1*123)\n",
    "        out = self.fc_layer(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.5 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], lter [20/90] Loss: 1.0207\n",
      "Epoch [1/10], lter [40/90] Loss: 0.9588\n",
      "Epoch [1/10], lter [60/90] Loss: 0.9262\n",
      "Epoch [1/10], lter [80/90] Loss: 0.9046\n",
      "Epoch [2/10], lter [20/90] Loss: 1.0553\n",
      "Epoch [2/10], lter [40/90] Loss: 1.3742\n",
      "Epoch [2/10], lter [60/90] Loss: 0.7051\n",
      "Epoch [2/10], lter [80/90] Loss: 0.9097\n",
      "Epoch [3/10], lter [20/90] Loss: 0.9631\n",
      "Epoch [3/10], lter [40/90] Loss: 0.9287\n",
      "Epoch [3/10], lter [60/90] Loss: 1.6013\n",
      "Epoch [3/10], lter [80/90] Loss: 0.9837\n",
      "Epoch [4/10], lter [20/90] Loss: 0.8085\n",
      "Epoch [4/10], lter [40/90] Loss: 1.1499\n",
      "Epoch [4/10], lter [60/90] Loss: 1.1496\n",
      "Epoch [4/10], lter [80/90] Loss: 0.8480\n",
      "Epoch [5/10], lter [20/90] Loss: 0.8915\n",
      "Epoch [5/10], lter [40/90] Loss: 0.7345\n",
      "Epoch [5/10], lter [60/90] Loss: 0.9784\n",
      "Epoch [5/10], lter [80/90] Loss: 1.2332\n",
      "Epoch [6/10], lter [20/90] Loss: 0.8985\n",
      "Epoch [6/10], lter [40/90] Loss: 0.5959\n",
      "Epoch [6/10], lter [60/90] Loss: 0.9041\n",
      "Epoch [6/10], lter [80/90] Loss: 1.0514\n",
      "Epoch [7/10], lter [20/90] Loss: 1.1154\n",
      "Epoch [7/10], lter [40/90] Loss: 0.8491\n",
      "Epoch [7/10], lter [60/90] Loss: 0.9606\n",
      "Epoch [7/10], lter [80/90] Loss: 0.9774\n",
      "Epoch [8/10], lter [20/90] Loss: 0.8281\n",
      "Epoch [8/10], lter [40/90] Loss: 0.8002\n",
      "Epoch [8/10], lter [60/90] Loss: 0.8154\n",
      "Epoch [8/10], lter [80/90] Loss: 0.9164\n",
      "Epoch [9/10], lter [20/90] Loss: 1.1309\n",
      "Epoch [9/10], lter [40/90] Loss: 1.1969\n",
      "Epoch [9/10], lter [60/90] Loss: 0.9171\n",
      "Epoch [9/10], lter [80/90] Loss: 1.0290\n",
      "Epoch [10/10], lter [20/90] Loss: 1.0920\n",
      "Epoch [10/10], lter [40/90] Loss: 0.7672\n",
      "Epoch [10/10], lter [60/90] Loss: 0.9390\n",
      "Epoch [10/10], lter [80/90] Loss: 0.8976\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, (batch_text, batch_labels) in enumerate(train_loader):\n",
    "        \n",
    "        X = Variable(batch_text).cuda()\n",
    "        Y = Variable(batch_labels).cuda()\n",
    "        \n",
    "        pre = model(X)\n",
    "        cost = loss(pre, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.data[0]))\n",
    "    \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.6 Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "\n",
    "test_data = data_utils.TensorDataset(x, y)\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test text: 58.149780 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for text, labels in test_loader:\n",
    "    \n",
    "    text = Variable(text).cuda()\n",
    "    outputs = model(text)\n",
    "    \n",
    "    _, pre = torch.max(outputs.data, 1)\n",
    "    total += 1\n",
    "    correct += (pre == labels.cuda()).sum()\n",
    "    \n",
    "print('Accuracy of test text: %f %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
