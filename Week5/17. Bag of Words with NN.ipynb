{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "import torch.nn.init\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.1 Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# G마켓에 좋은 평만 있어서 사봤는데.. 정말 진짜 진짜 사지마세요. 개. 쓰. 레. 기 (진심) 입니다. 액정부터 짜증나는 TN패널에, 하드 SSD인걸로 알았는데, 속도는 저질 SD카드 꽂아 놓은것 같습니다. 정말 느려터집니다. 저는 단지 인터넷 뱅킹만 할려고, 샀단 말입니다. 그런데 인터넷 뱅킹 프로그램까는데만 10~20분 걸립니다. 뭐약!! 이게!! 분노로 인해 볼때마다 짜증납니다. 밤에 잠도 안오고요.. 사시면 분명 후회하실겁니다. 아! 진짜 G마켓 프리미엄평으로 실날하게 사진찍어서 올리려고했는데, 먹고 산다고 바빠서 프리미엄 평 못 올린게 정말 천추의 한이네요!!\\n# 원래 그런 줄 알고 사는 \"저가 제품\"이라고 생각합니다만. IPS라는 언급이 없으니 당연히 TN 패널일 테고, EMMC는 SSD가 아니고 SD 카드 내장된 것 같은 것이라 원래 SSD보다 느린 것이고, CPU도 아톰이니 뭐 당연히 느리죠. 그런 것 다 감안하고 \"싸고 가볍다\"는 조건으로 사는 제품인데요. 뭐 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = codecs.open(\"data/reviews.txt\", 'r', 'utf-8')\n",
    "f.read()[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.2 Extract Setences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"발열\", \"소음\"]\n",
    "\n",
    "for keyword in keywords :\n",
    "    temp_list = []\n",
    "    save_name = \"data/reviews_\" + keyword + \".txt\"\n",
    "    f = codecs.open(\"data/reviews.txt\", 'r', 'utf-8')\n",
    "    t = codecs.open(save_name, 'w', 'utf-8')\n",
    "    \n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line: break\n",
    "        if keyword in line :\n",
    "            temp_list.append(line)\n",
    "            \n",
    "    set_list = list(set(temp_list))\n",
    "    \n",
    "    for item in set_list :\n",
    "        t.write(item)\n",
    "        \n",
    "    f.close()\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 리오나인 게이밍노트북 L7S-16H 가성비가 좋습니다 발열은 그렇게\\n# 배송 빠릅니다..1~2일내 받아볼수 있어요..포장도 안전하게 잘되어있고 부팅속도 빠릅니다..Hdd 1t 추가해서 저장용량도 넉넉함..윈도10 설치옵션 했는데 잘 깔려있네요..이가격 대비 이정도 성능이면 비교 우위라 할 수 있겠죠..여러 제품 비교해 보고 주문했는데여러모로 적정한 성능.. 가격.. 나름 만족스러운 편..아들이 오버워치할 생각에 좋아하네요..도 잘되고 가성비 최고네요속도, 소음, 발열 아직까진 무난하네요.제품 받고 문의했는데 바로 통화되서 해결..이 점도 가점 줄 수 있을 듯...다만 마우스라도 서비스 주면 좋았을텐데..노트북\\n# 회사업무용도로 얼마 전 hp 오멘 i7을 구입했었습니다.이번 acer 노트북은 개인적으로 쓰고자 구입하였는데윈도우10을 설치하여 잠시 디아블로 및 사진편집등 테스트해보니 프로그램 구동속도도 빠르고 오멘과 비교하여도 손색이 없었으며 구조적인 부분도 신경써서설계된듯한 느낌을'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = codecs.open(\"data/reviews_발열.txt\", 'r', 'utf-8')\n",
    "f.read()[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.3 Load Scored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"data/score_발열.xlsx\"\n",
    "sheet_name = \"Sheet1\"\n",
    "data = pd.read_excel(filename, sheet_name = sheet_name, header = 0)\n",
    "\n",
    "csv_data = [item.replace(\"#\", \"\").strip() for item in data['Review']]\n",
    "csv_label = data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['발열히 심한거 같은데 여름이라 그런가?..',\n",
       " '발열이좀 심한거 같아서 걱정이에요',\n",
       " '발열이심하더라구요',\n",
       " '발열이너무심한게 제일큰 단점인것 같고 그외에 불편한점은',\n",
       " '발열이...정말...심합니다']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2680b0d0358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = [sum(csv_label == 0), sum(csv_label == 1), sum(csv_label == 2)]\n",
    "plt.bar(range(len(scores)),scores, color=\"#ff5733\")\n",
    "\n",
    "plt.title(\"Average Laptop's Score of Heat\")\n",
    "plt.xlabel(\"Heat Score\")\n",
    "plt.ylabel(\"Reviews\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.4 Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twitter()\n",
    "doc = []\n",
    "\n",
    "for sentence in csv_data :\n",
    "    results= []\n",
    "    tokens = twitter.pos(sentence, norm=True, stem=True)\n",
    "        \n",
    "    for token in tokens:\n",
    "        if not token[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            results.append(token[0])\n",
    "    doc.append(\" \".join(results).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['발열 히 심하다 같다 여름 그 런가',\n",
       " '발열 이 좀 심하다 같다 걱정',\n",
       " '발열 심하다',\n",
       " '발열 이 너무 심하다 제일 크다 단점 것 같다 그 외 불편하다 점',\n",
       " '발열 정말 심하다']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "cnv = CountVectorizer(ngram_range=(1,1), min_df = 3)\n",
    "\n",
    "data = cnv.fit_transform(doc).toarray()\n",
    "label = csv_label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorizer\n",
    "# tfidv = TfidfVectorizer().fit(data)\n",
    "# data = tfidv.transform(data).toarray()\n",
    "\n",
    "# data = pd.DataFrame(data)\n",
    "# data = (data - data.mean()) / (data.max() - data.min() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': 0,\n",
       " '100': 1,\n",
       " '50': 2,\n",
       " '70': 3,\n",
       " 'as': 4,\n",
       " 'cpu': 5,\n",
       " 'ips': 6,\n",
       " 'ssd': 7,\n",
       " 'ㅜㅜ': 8,\n",
       " 'ㅠㅠ': 9,\n",
       " '가격': 10,\n",
       " '가다': 11,\n",
       " '가볍다': 12,\n",
       " '가성': 13,\n",
       " '감다': 14,\n",
       " '같다': 15,\n",
       " '개선': 16,\n",
       " '거의': 17,\n",
       " '걱정': 18,\n",
       " '걸리다': 19,\n",
       " '게임': 20,\n",
       " '겨울': 21,\n",
       " '고민': 22,\n",
       " '고사': 23,\n",
       " '관리': 24,\n",
       " '괜찮다': 25,\n",
       " '굉장하다': 26,\n",
       " '구매': 27,\n",
       " '구성': 28,\n",
       " '그냥': 29,\n",
       " '그래도': 30,\n",
       " '그래픽': 31,\n",
       " '그램': 32,\n",
       " '그렇게': 33,\n",
       " '그렇다': 34,\n",
       " '그리다': 35,\n",
       " '기능': 36,\n",
       " '기다': 37,\n",
       " '기본': 38,\n",
       " '까지': 39,\n",
       " '깔끔하다': 40,\n",
       " '끊기다': 41,\n",
       " '나다': 42,\n",
       " '나쁘다': 43,\n",
       " '나오다': 44,\n",
       " '낮다': 45,\n",
       " '내다': 46,\n",
       " '너무': 47,\n",
       " '노트': 48,\n",
       " '노트북': 49,\n",
       " '놀라다': 50,\n",
       " '높다': 51,\n",
       " '느껴지다': 52,\n",
       " '느끼다': 53,\n",
       " '느낌': 54,\n",
       " '느리다': 55,\n",
       " '늘다': 56,\n",
       " '능력': 57,\n",
       " '다니다': 58,\n",
       " '다른': 59,\n",
       " '다소': 60,\n",
       " '단점': 61,\n",
       " '대다': 62,\n",
       " '대단하다': 63,\n",
       " '대만': 64,\n",
       " '대비': 65,\n",
       " '던지다': 66,\n",
       " '데스크탑': 67,\n",
       " '돌다': 68,\n",
       " '돌리다': 69,\n",
       " '돌아가다': 70,\n",
       " '돼다': 71,\n",
       " '되다': 72,\n",
       " '되어다': 73,\n",
       " '두다': 74,\n",
       " '들다': 75,\n",
       " '듭니': 76,\n",
       " '등등': 77,\n",
       " '디자인': 78,\n",
       " '따다': 79,\n",
       " '따뜻하다': 80,\n",
       " '따르다': 81,\n",
       " '딱하다': 82,\n",
       " '때문': 83,\n",
       " '떨어지다': 84,\n",
       " '또한': 85,\n",
       " '뛰어나다': 86,\n",
       " '뜨겁다': 87,\n",
       " '뜨다': 88,\n",
       " '라면': 89,\n",
       " '레노버': 90,\n",
       " '마감': 91,\n",
       " '마음': 92,\n",
       " '만족': 93,\n",
       " '만족스럽다': 94,\n",
       " '많다': 95,\n",
       " '많이': 96,\n",
       " '매우': 97,\n",
       " '메탈': 98,\n",
       " '모니터': 99,\n",
       " '모델': 100,\n",
       " '모두': 101,\n",
       " '모든': 102,\n",
       " '모르다': 103,\n",
       " '무겁다': 104,\n",
       " '무게': 105,\n",
       " '무리': 106,\n",
       " '무엇': 107,\n",
       " '문서': 108,\n",
       " '문제': 109,\n",
       " '물론': 110,\n",
       " '미미': 111,\n",
       " '받다': 112,\n",
       " '받침': 113,\n",
       " '발생': 114,\n",
       " '발열': 115,\n",
       " '밧데리': 116,\n",
       " '배그': 117,\n",
       " '배송': 118,\n",
       " '배터리': 119,\n",
       " '별로': 120,\n",
       " '보다': 121,\n",
       " '보드': 122,\n",
       " '보이다': 123,\n",
       " '보통': 124,\n",
       " '부분': 125,\n",
       " '부팅': 126,\n",
       " '불량': 127,\n",
       " '불편하다': 128,\n",
       " '비추다': 129,\n",
       " '빠르다': 130,\n",
       " '빠릿빠릿': 131,\n",
       " '빠지다': 132,\n",
       " '빼다': 133,\n",
       " '사다': 134,\n",
       " '사양': 135,\n",
       " '사용': 136,\n",
       " '사은': 137,\n",
       " '살짝': 138,\n",
       " '삼성': 139,\n",
       " '상당하다': 140,\n",
       " '상태': 141,\n",
       " '생각': 142,\n",
       " '생기다': 143,\n",
       " '서비스': 144,\n",
       " '선택': 145,\n",
       " '설치': 146,\n",
       " '성능': 147,\n",
       " '센터': 148,\n",
       " '소리': 149,\n",
       " '소모': 150,\n",
       " '소음': 151,\n",
       " '속도': 152,\n",
       " '수준': 153,\n",
       " '시간': 154,\n",
       " '신경': 155,\n",
       " '심다': 156,\n",
       " '심하다': 157,\n",
       " '심해': 158,\n",
       " '심해지다': 159,\n",
       " '쓰다': 160,\n",
       " '쓰이다': 161,\n",
       " '쓸다': 162,\n",
       " '아니다': 163,\n",
       " '아쉽다': 164,\n",
       " '아예': 165,\n",
       " '아주': 166,\n",
       " '아직': 167,\n",
       " '안나': 168,\n",
       " '안심': 169,\n",
       " '않다': 170,\n",
       " '알다': 171,\n",
       " '액정': 172,\n",
       " '야하다': 173,\n",
       " '약간': 174,\n",
       " '약하다': 175,\n",
       " '얇다': 176,\n",
       " '양호': 177,\n",
       " '얘기': 178,\n",
       " '어느': 179,\n",
       " '어떻다': 180,\n",
       " '어쩌다': 181,\n",
       " '엄청': 182,\n",
       " '엄청나다': 183,\n",
       " '없다': 184,\n",
       " '없이': 185,\n",
       " '에서': 186,\n",
       " '여름': 187,\n",
       " '역시': 188,\n",
       " '연결': 189,\n",
       " '열량': 190,\n",
       " '예쁘다': 191,\n",
       " '오다': 192,\n",
       " '오래': 193,\n",
       " '온도': 194,\n",
       " '완벽하다': 195,\n",
       " '완전': 196,\n",
       " '외관': 197,\n",
       " '왼쪽': 198,\n",
       " '용량': 199,\n",
       " '우수하다': 200,\n",
       " '원래': 201,\n",
       " '윈도우': 202,\n",
       " '이다': 203,\n",
       " '이렇게': 204,\n",
       " '이렇다': 205,\n",
       " '이면': 206,\n",
       " '이쁘다': 207,\n",
       " '이상': 208,\n",
       " '이슈': 209,\n",
       " '이전': 210,\n",
       " '이정': 211,\n",
       " '이제': 212,\n",
       " '인터넷': 213,\n",
       " '일어나서': 214,\n",
       " '일이': 215,\n",
       " '있다': 216,\n",
       " '자다': 217,\n",
       " '자주': 218,\n",
       " '자판': 219,\n",
       " '작다': 220,\n",
       " '작성': 221,\n",
       " '작업': 222,\n",
       " '잘되다': 223,\n",
       " '잡고': 224,\n",
       " '잡다': 225,\n",
       " '잡지': 226,\n",
       " '잡히다': 227,\n",
       " '장시간': 228,\n",
       " '장점': 229,\n",
       " '저렴': 230,\n",
       " '적다': 231,\n",
       " '적당하다': 232,\n",
       " '전혀': 233,\n",
       " '절대': 234,\n",
       " '점수': 235,\n",
       " '정도': 236,\n",
       " '정말': 237,\n",
       " '제어': 238,\n",
       " '제외': 239,\n",
       " '제품': 240,\n",
       " '조금': 241,\n",
       " '조용하다': 242,\n",
       " '조절': 243,\n",
       " '좋다': 244,\n",
       " '주다': 245,\n",
       " '준수': 246,\n",
       " '증상': 247,\n",
       " '지금': 248,\n",
       " '지다': 249,\n",
       " '진짜': 250,\n",
       " '짧다': 251,\n",
       " '차다': 252,\n",
       " '차이': 253,\n",
       " '처리': 254,\n",
       " '처음': 255,\n",
       " '최고': 256,\n",
       " '추천': 257,\n",
       " '충전': 258,\n",
       " '치다': 259,\n",
       " '카드': 260,\n",
       " '컴퓨터': 261,\n",
       " '켜다': 262,\n",
       " '쿨러': 263,\n",
       " '쿨링': 264,\n",
       " '크게': 265,\n",
       " '크다': 266,\n",
       " '특히': 267,\n",
       " '틀림': 268,\n",
       " '패널': 269,\n",
       " '패드': 270,\n",
       " '펴다': 271,\n",
       " '편이': 272,\n",
       " '편입': 273,\n",
       " '편하다': 274,\n",
       " '평소': 275,\n",
       " '프로그램': 276,\n",
       " '필요': 277,\n",
       " '하나': 278,\n",
       " '하니': 279,\n",
       " '하다': 280,\n",
       " '하드': 281,\n",
       " '하지만': 282,\n",
       " '하판': 283,\n",
       " '한성': 284,\n",
       " '한편': 285,\n",
       " '해보다': 286,\n",
       " '해소': 287,\n",
       " '현상': 288,\n",
       " '현재': 289,\n",
       " '화면': 290,\n",
       " '화상': 291,\n",
       " '화이트': 292,\n",
       " '화질': 293,\n",
       " '확실하다': 294,\n",
       " '훨씬': 295,\n",
       " '휴대': 296}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.5 Train-Test Spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(cnv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "908\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_label, test_label = train_test_split(data, label, stratify = label)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(train_data).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(train_label).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([908]), torch.Size([908, 297]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size() , x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_utils.TensorDataset(x, y)\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1,\n",
    "                                          drop_last = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.6 Define and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = torch.nn.Linear(dim, 200, bias = True)\n",
    "linear2 = torch.nn.Linear(200, 3, bias = True)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "model = torch.nn.Sequential(linear1, relu, linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], lter [20/90] Loss: 0.9353\n",
      "Epoch [1/50], lter [40/90] Loss: 1.0259\n",
      "Epoch [1/50], lter [60/90] Loss: 1.1118\n",
      "Epoch [1/50], lter [80/90] Loss: 0.4920\n",
      "Epoch [2/50], lter [20/90] Loss: 0.6720\n",
      "Epoch [2/50], lter [40/90] Loss: 0.4913\n",
      "Epoch [2/50], lter [60/90] Loss: 0.5884\n",
      "Epoch [2/50], lter [80/90] Loss: 0.0658\n",
      "Epoch [3/50], lter [20/90] Loss: 0.3164\n",
      "Epoch [3/50], lter [40/90] Loss: 0.6434\n",
      "Epoch [3/50], lter [60/90] Loss: 0.5466\n",
      "Epoch [3/50], lter [80/90] Loss: 0.2428\n",
      "Epoch [4/50], lter [20/90] Loss: 0.2200\n",
      "Epoch [4/50], lter [40/90] Loss: 0.1370\n",
      "Epoch [4/50], lter [60/90] Loss: 0.1123\n",
      "Epoch [4/50], lter [80/90] Loss: 0.2295\n",
      "Epoch [5/50], lter [20/90] Loss: 0.2069\n",
      "Epoch [5/50], lter [40/90] Loss: 0.2534\n",
      "Epoch [5/50], lter [60/90] Loss: 0.1759\n",
      "Epoch [5/50], lter [80/90] Loss: 0.0494\n",
      "Epoch [6/50], lter [20/90] Loss: 0.3419\n",
      "Epoch [6/50], lter [40/90] Loss: 0.3530\n",
      "Epoch [6/50], lter [60/90] Loss: 0.1012\n",
      "Epoch [6/50], lter [80/90] Loss: 0.1337\n",
      "Epoch [7/50], lter [20/90] Loss: 0.1737\n",
      "Epoch [7/50], lter [40/90] Loss: 0.0495\n",
      "Epoch [7/50], lter [60/90] Loss: 0.2505\n",
      "Epoch [7/50], lter [80/90] Loss: 0.2720\n",
      "Epoch [8/50], lter [20/90] Loss: 0.3988\n",
      "Epoch [8/50], lter [40/90] Loss: 0.0507\n",
      "Epoch [8/50], lter [60/90] Loss: 0.2228\n",
      "Epoch [8/50], lter [80/90] Loss: 0.1469\n",
      "Epoch [9/50], lter [20/90] Loss: 0.0642\n",
      "Epoch [9/50], lter [40/90] Loss: 0.2387\n",
      "Epoch [9/50], lter [60/90] Loss: 0.1442\n",
      "Epoch [9/50], lter [80/90] Loss: 0.0433\n",
      "Epoch [10/50], lter [20/90] Loss: 0.0098\n",
      "Epoch [10/50], lter [40/90] Loss: 0.0810\n",
      "Epoch [10/50], lter [60/90] Loss: 0.0847\n",
      "Epoch [10/50], lter [80/90] Loss: 0.4126\n",
      "Epoch [11/50], lter [20/90] Loss: 0.0299\n",
      "Epoch [11/50], lter [40/90] Loss: 0.0499\n",
      "Epoch [11/50], lter [60/90] Loss: 0.1375\n",
      "Epoch [11/50], lter [80/90] Loss: 0.0901\n",
      "Epoch [12/50], lter [20/90] Loss: 0.0279\n",
      "Epoch [12/50], lter [40/90] Loss: 0.0440\n",
      "Epoch [12/50], lter [60/90] Loss: 0.2954\n",
      "Epoch [12/50], lter [80/90] Loss: 0.1203\n",
      "Epoch [13/50], lter [20/90] Loss: 0.1144\n",
      "Epoch [13/50], lter [40/90] Loss: 0.1660\n",
      "Epoch [13/50], lter [60/90] Loss: 0.0117\n",
      "Epoch [13/50], lter [80/90] Loss: 0.0260\n",
      "Epoch [14/50], lter [20/90] Loss: 0.0585\n",
      "Epoch [14/50], lter [40/90] Loss: 0.0382\n",
      "Epoch [14/50], lter [60/90] Loss: 0.1313\n",
      "Epoch [14/50], lter [80/90] Loss: 0.1404\n",
      "Epoch [15/50], lter [20/90] Loss: 0.0189\n",
      "Epoch [15/50], lter [40/90] Loss: 0.1914\n",
      "Epoch [15/50], lter [60/90] Loss: 0.1660\n",
      "Epoch [15/50], lter [80/90] Loss: 0.0468\n",
      "Epoch [16/50], lter [20/90] Loss: 0.0254\n",
      "Epoch [16/50], lter [40/90] Loss: 0.1188\n",
      "Epoch [16/50], lter [60/90] Loss: 0.0114\n",
      "Epoch [16/50], lter [80/90] Loss: 0.0295\n",
      "Epoch [17/50], lter [20/90] Loss: 0.1205\n",
      "Epoch [17/50], lter [40/90] Loss: 0.0235\n",
      "Epoch [17/50], lter [60/90] Loss: 0.0104\n",
      "Epoch [17/50], lter [80/90] Loss: 0.0391\n",
      "Epoch [18/50], lter [20/90] Loss: 0.0498\n",
      "Epoch [18/50], lter [40/90] Loss: 0.0208\n",
      "Epoch [18/50], lter [60/90] Loss: 0.0771\n",
      "Epoch [18/50], lter [80/90] Loss: 0.0081\n",
      "Epoch [19/50], lter [20/90] Loss: 0.0271\n",
      "Epoch [19/50], lter [40/90] Loss: 0.0795\n",
      "Epoch [19/50], lter [60/90] Loss: 0.0014\n",
      "Epoch [19/50], lter [80/90] Loss: 0.0161\n",
      "Epoch [20/50], lter [20/90] Loss: 0.0806\n",
      "Epoch [20/50], lter [40/90] Loss: 0.0322\n",
      "Epoch [20/50], lter [60/90] Loss: 0.0266\n",
      "Epoch [20/50], lter [80/90] Loss: 0.0044\n",
      "Epoch [21/50], lter [20/90] Loss: 0.0509\n",
      "Epoch [21/50], lter [40/90] Loss: 0.0341\n",
      "Epoch [21/50], lter [60/90] Loss: 0.0583\n",
      "Epoch [21/50], lter [80/90] Loss: 0.0947\n",
      "Epoch [22/50], lter [20/90] Loss: 0.1076\n",
      "Epoch [22/50], lter [40/90] Loss: 0.1085\n",
      "Epoch [22/50], lter [60/90] Loss: 0.0809\n",
      "Epoch [22/50], lter [80/90] Loss: 0.0123\n",
      "Epoch [23/50], lter [20/90] Loss: 0.0203\n",
      "Epoch [23/50], lter [40/90] Loss: 0.0689\n",
      "Epoch [23/50], lter [60/90] Loss: 0.0065\n",
      "Epoch [23/50], lter [80/90] Loss: 0.0711\n",
      "Epoch [24/50], lter [20/90] Loss: 0.0326\n",
      "Epoch [24/50], lter [40/90] Loss: 0.0209\n",
      "Epoch [24/50], lter [60/90] Loss: 0.0630\n",
      "Epoch [24/50], lter [80/90] Loss: 0.0084\n",
      "Epoch [25/50], lter [20/90] Loss: 0.2815\n",
      "Epoch [25/50], lter [40/90] Loss: 0.0080\n",
      "Epoch [25/50], lter [60/90] Loss: 0.0020\n",
      "Epoch [25/50], lter [80/90] Loss: 0.1248\n",
      "Epoch [26/50], lter [20/90] Loss: 0.0181\n",
      "Epoch [26/50], lter [40/90] Loss: 0.0186\n",
      "Epoch [26/50], lter [60/90] Loss: 0.1882\n",
      "Epoch [26/50], lter [80/90] Loss: 0.0133\n",
      "Epoch [27/50], lter [20/90] Loss: 0.0508\n",
      "Epoch [27/50], lter [40/90] Loss: 0.0039\n",
      "Epoch [27/50], lter [60/90] Loss: 0.2238\n",
      "Epoch [27/50], lter [80/90] Loss: 0.0032\n",
      "Epoch [28/50], lter [20/90] Loss: 0.0618\n",
      "Epoch [28/50], lter [40/90] Loss: 0.0046\n",
      "Epoch [28/50], lter [60/90] Loss: 0.1045\n",
      "Epoch [28/50], lter [80/90] Loss: 0.0315\n",
      "Epoch [29/50], lter [20/90] Loss: 0.0107\n",
      "Epoch [29/50], lter [40/90] Loss: 0.1778\n",
      "Epoch [29/50], lter [60/90] Loss: 0.0046\n",
      "Epoch [29/50], lter [80/90] Loss: 0.0387\n",
      "Epoch [30/50], lter [20/90] Loss: 0.0105\n",
      "Epoch [30/50], lter [40/90] Loss: 0.0033\n",
      "Epoch [30/50], lter [60/90] Loss: 0.0241\n",
      "Epoch [30/50], lter [80/90] Loss: 0.0083\n",
      "Epoch [31/50], lter [20/90] Loss: 0.0231\n",
      "Epoch [31/50], lter [40/90] Loss: 0.1208\n",
      "Epoch [31/50], lter [60/90] Loss: 0.0008\n",
      "Epoch [31/50], lter [80/90] Loss: 0.0136\n",
      "Epoch [32/50], lter [20/90] Loss: 0.0062\n",
      "Epoch [32/50], lter [40/90] Loss: 0.0182\n",
      "Epoch [32/50], lter [60/90] Loss: 0.0197\n",
      "Epoch [32/50], lter [80/90] Loss: 0.0402\n",
      "Epoch [33/50], lter [20/90] Loss: 0.0087\n",
      "Epoch [33/50], lter [40/90] Loss: 0.0098\n",
      "Epoch [33/50], lter [60/90] Loss: 0.0011\n",
      "Epoch [33/50], lter [80/90] Loss: 0.0038\n",
      "Epoch [34/50], lter [20/90] Loss: 0.0062\n",
      "Epoch [34/50], lter [40/90] Loss: 0.0891\n",
      "Epoch [34/50], lter [60/90] Loss: 0.0229\n",
      "Epoch [34/50], lter [80/90] Loss: 0.0609\n",
      "Epoch [35/50], lter [20/90] Loss: 0.0068\n",
      "Epoch [35/50], lter [40/90] Loss: 0.0026\n",
      "Epoch [35/50], lter [60/90] Loss: 0.0295\n",
      "Epoch [35/50], lter [80/90] Loss: 0.0029\n",
      "Epoch [36/50], lter [20/90] Loss: 0.0049\n",
      "Epoch [36/50], lter [40/90] Loss: 0.0234\n",
      "Epoch [36/50], lter [60/90] Loss: 0.0044\n",
      "Epoch [36/50], lter [80/90] Loss: 0.0178\n",
      "Epoch [37/50], lter [20/90] Loss: 0.0368\n",
      "Epoch [37/50], lter [40/90] Loss: 0.0028\n",
      "Epoch [37/50], lter [60/90] Loss: 0.4226\n",
      "Epoch [37/50], lter [80/90] Loss: 0.0045\n",
      "Epoch [38/50], lter [20/90] Loss: 0.0012\n",
      "Epoch [38/50], lter [40/90] Loss: 0.2091\n",
      "Epoch [38/50], lter [60/90] Loss: 0.0106\n",
      "Epoch [38/50], lter [80/90] Loss: 0.0444\n",
      "Epoch [39/50], lter [20/90] Loss: 0.2365\n",
      "Epoch [39/50], lter [40/90] Loss: 0.0082\n",
      "Epoch [39/50], lter [60/90] Loss: 0.1309\n",
      "Epoch [39/50], lter [80/90] Loss: 0.0749\n",
      "Epoch [40/50], lter [20/90] Loss: 0.0094\n",
      "Epoch [40/50], lter [40/90] Loss: 0.0055\n",
      "Epoch [40/50], lter [60/90] Loss: 0.0157\n",
      "Epoch [40/50], lter [80/90] Loss: 0.0280\n",
      "Epoch [41/50], lter [20/90] Loss: 0.0153\n",
      "Epoch [41/50], lter [40/90] Loss: 0.0022\n",
      "Epoch [41/50], lter [60/90] Loss: 0.0341\n",
      "Epoch [41/50], lter [80/90] Loss: 0.0085\n",
      "Epoch [42/50], lter [20/90] Loss: 0.0293\n",
      "Epoch [42/50], lter [40/90] Loss: 0.0030\n",
      "Epoch [42/50], lter [60/90] Loss: 0.1446\n",
      "Epoch [42/50], lter [80/90] Loss: 0.0048\n",
      "Epoch [43/50], lter [20/90] Loss: 0.0056\n",
      "Epoch [43/50], lter [40/90] Loss: 0.0037\n",
      "Epoch [43/50], lter [60/90] Loss: 0.0016\n",
      "Epoch [43/50], lter [80/90] Loss: 0.0720\n",
      "Epoch [44/50], lter [20/90] Loss: 0.0049\n",
      "Epoch [44/50], lter [40/90] Loss: 0.0614\n",
      "Epoch [44/50], lter [60/90] Loss: 0.0028\n",
      "Epoch [44/50], lter [80/90] Loss: 0.0021\n",
      "Epoch [45/50], lter [20/90] Loss: 0.0097\n",
      "Epoch [45/50], lter [40/90] Loss: 0.0025\n",
      "Epoch [45/50], lter [60/90] Loss: 0.0361\n",
      "Epoch [45/50], lter [80/90] Loss: 0.0272\n",
      "Epoch [46/50], lter [20/90] Loss: 0.0007\n",
      "Epoch [46/50], lter [40/90] Loss: 0.0211\n",
      "Epoch [46/50], lter [60/90] Loss: 0.0681\n",
      "Epoch [46/50], lter [80/90] Loss: 0.0153\n",
      "Epoch [47/50], lter [20/90] Loss: 0.0061\n",
      "Epoch [47/50], lter [40/90] Loss: 0.0112\n",
      "Epoch [47/50], lter [60/90] Loss: 0.0023\n",
      "Epoch [47/50], lter [80/90] Loss: 0.1537\n",
      "Epoch [48/50], lter [20/90] Loss: 0.0287\n",
      "Epoch [48/50], lter [40/90] Loss: 0.1401\n",
      "Epoch [48/50], lter [60/90] Loss: 0.0035\n",
      "Epoch [48/50], lter [80/90] Loss: 0.0019\n",
      "Epoch [49/50], lter [20/90] Loss: 0.0241\n",
      "Epoch [49/50], lter [40/90] Loss: 0.0065\n",
      "Epoch [49/50], lter [60/90] Loss: 0.1237\n",
      "Epoch [49/50], lter [80/90] Loss: 0.1002\n",
      "Epoch [50/50], lter [20/90] Loss: 0.0048\n",
      "Epoch [50/50], lter [40/90] Loss: 0.0054\n",
      "Epoch [50/50], lter [60/90] Loss: 0.0029\n",
      "Epoch [50/50], lter [80/90] Loss: 0.0013\n",
      "Learning Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_data) // batch_size\n",
    "    \n",
    "    for i, (batch_text, batch_labels) in enumerate(train_loader):\n",
    "        \n",
    "        X = Variable(batch_text.view(-1, dim))\n",
    "        Y = Variable(batch_labels)\n",
    "        \n",
    "        pre = model(X)\n",
    "        cost = loss(pre, Y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 20 == 0:\n",
    "            print('Epoch [%d/%d], lter [%d/%d] Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, total_batch, cost.data[0]))\n",
    "    \n",
    "print(\"Learning Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.7 Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(test_data).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(test_label).type(torch.LongTensor)\n",
    "\n",
    "test_data = data_utils.TensorDataset(x, y)\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test text: 98.678414 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for text, labels in test_loader:\n",
    "    \n",
    "    text = Variable(text.view(-1, dim))\n",
    "    outputs = model(text)\n",
    "    \n",
    "    _, pre = torch.max(outputs.data, 1)\n",
    "    total += 1\n",
    "    correct += (pre == labels).sum()\n",
    "    \n",
    "print('Accuracy of test text: %f %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.8 Test Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsitHot(string) :\n",
    "    \n",
    "    results= []\n",
    "    tokens = twitter.pos(string, norm=True, stem=True)\n",
    "\n",
    "    for token in tokens:\n",
    "        if not token[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "            results.append(token[0])\n",
    "   \n",
    "    sample = cnv.transform([\" \".join(results).strip()]).toarray()\n",
    "    sample = torch.from_numpy(sample).type(torch.FloatTensor)\n",
    "    res = torch.max(model(Variable(sample)).data, 1)[1].numpy()\n",
    "    \n",
    "    if res == 0 : \n",
    "        print(\"분석 결과 : 발열 거의 없음\")\n",
    "    elif res == 1 :\n",
    "        print(\"분석 결과 : 발열 조금 있음\")\n",
    "    else :\n",
    "        print(\"분석 결과 : 발열 매우 심함\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 결과 : 발열 매우 심함\n"
     ]
    }
   ],
   "source": [
    "IsitHot(\"노트북이 너무 뜨거워요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분석 결과 : 발열 거의 없음\n"
     ]
    }
   ],
   "source": [
    "IsitHot(\"발열을 잘 잡았네요\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
